#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup as bs
import requests
import pymongo
import random
import time

#连接数据库
connclint = pymongo.MongoClient('127.0.0.1:27017')
db = connclint['36dsj']

headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0'
}
content_url = 'http://www.36dsj.com/archives/1078'
list_url = 'http://www.36dsj.com/page/30'
list_urls = ['http://www.36dsj.com/page/{}'.format(str(i)) for i in range(1,500,1)]

#获取URL连接
def get_url_list(url):
    l = []
    list_data = requests.get(url,headers=headers)
    soup = bs(list_data.text,'lxml')
    if soup.find('article',class_ = 'excerpt'):
        links = soup.select('div.content article.excerpt h2 a')
        for link in links:
            link = link.get('href')
            l.append(link)
            # print(link)
            # return l
    else:
        pass;
    return  l
# s = []
# for single_url in list_urls:
#     s = get_url_list(single_url)
#     for i in s:
#         print(i)

def get_content():
    a = {}
    t = 0
    for single_url in list_urls:
        tar_list =get_url_list(single_url)
        for tar in tar_list:
            web_data = requests.get(tar,headers=headers)
            soup = bs(web_data.text,'lxml')
            time.sleep(int(random.randint(0,2)))
            title = soup.select('div.content h1.article-title a')[0].get_text()
            author = soup.select('div.content header.article-header ul.article-meta li')[0].get_text()
            posttime = soup.select('div.content header.article-header ul.article-meta li')[1].get_text()
            cate = soup.select('div.content header.article-header ul.article-meta li')[2].get_text()
            rate = soup.select('div.content header.article-header ul.article-meta li')[3].get_text()
            content = soup.select('div.content article.article-content')[0].get_text().strip()
            a = {
                'link':tar,
                'title':title,
                'author':author,
                'posttime':posttime,
                'cate':cate,
                'rate':rate,
                'content':content
            }
            db.contentdsj.insert(a)
            t += 1
            print(t)
# get_content(content_url)
get_content()
