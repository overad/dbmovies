# /usr/bin/python
# encoding:utf-8

from bs4 import BeautifulSoup as bs 
import requests
import time
import os
import random
import urllib.request

urls = ['http://www.swmt.cc/t/rentiyishutupian/{}.html'.format(str(i)) for i in range(2,3,1)]



headers = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'
}

proxy_list = [
	{'http':"222.240.233.154:80"},
	{'http':"182.88.126.8:8118"},
	{'http':"121.204.165.18:8118"},
	{'http':"125.125.223.166:10000"},
	{'http':"116.213.98.6:8080"}
]

proxies = random.choice(proxy_list)

host = 'http://www.swmt.cc'

def get_list_t(url):
	l = []
	web_data = requests.get(url,headers=headers,proxies= proxies)
	web_data.encoding = 'utf-8'
	soup = bs(web_data.text,'lxml')
	imgs = soup.select('div.boxs ul.img li a img')
	titles = soup.select('div.boxs > ul.img li p.p_title a')
	links = soup.select('div.boxs > ul.img li p.p_title a')

	for img,title,link in zip(imgs,titles, links):
		img = img['src']
		title = title.get_text().strip()
		link = host + link.get('href')
		l.append([img,title,link])
	return l

# print(get_list_t(url))


#遍历内容页文件
def get_content_imgs(url):
	i = 1
	abd = []
	time.sleep(1)
	for ks in get_list_t(url):
		title = ks[1]
		link = ks[2]
		webd = requests.get(link,headers=headers,proxies = proxies)
		webd.encoding = 'utf-8'
		soup = bs(webd.text,'lxml')
		imgs = soup.select('div.content center p a img')
		for img in imgs:
			img = img['src']
			# print(i,img)
			abd.append([title,img])
			i += 1
	return abd

# print(get_content_imgs(url))

def downloading_list_img(url):
	path = 'E:\pachonglibs\swmt\\'
	os.chdir(path)
	file_lists = os.listdir()
	# print(file_lists)

	#遍历文件名;
	for i in get_content_imgs(url):
		img = i[1]
		title = i[0]
		name = img.split('/')[-1].split('-')[0]
		print(name,img,title)
		if os.path.exists(title):
			print(title)
			urllib.request.urlretrieve(img,path+title+'\%s.jpg'%name)
		else:
			os.mkdir(title)
			urllib.request.urlretrieve(img,path+title+'\%s.jpg'%name)
	print(os.getcwd())

# downloading_list_img(url)

for single_url in urls:
	downloading_list_img(single_url)
